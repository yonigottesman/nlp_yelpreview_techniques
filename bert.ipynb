{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind of a simplidied version of glue.py and run_glue.py from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "from torchtext import datasets\n",
    "import time\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert imports\n",
    "# To import BertTokenizerFast i must install from src\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers import BertForSequenceClassification,BertModel,BertConfig,BertTokenizerFast,BertTokenizer,AdamW,get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x yelp_review_polarity_csv/\n",
      "x yelp_review_polarity_csv/readme.txt\n",
      "x yelp_review_polarity_csv/test.csv\n",
      "x yelp_review_polarity_csv/train.csv\n"
     ]
    }
   ],
   "source": [
    "# Download yelp  polarity\n",
    "torchtext.utils.download_from_url(datasets.text_classification.URLS['YelpReviewPolarity'])\n",
    "!tar -C .data -xvf .data/yelp_review_polarity_csv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('.data/yelp_review_polarity_csv/train.csv',names=['label','text'])\n",
    "test_df = pd.read_csv('.data/yelp_review_polarity_csv/test.csv',names=['label','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name='bert-base-uncased'\n",
    "bs=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO YONIGO - where is this cached?\n",
    "config = BertConfig.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples,\n",
    "                                 labels,\n",
    "                                 tokenizer,          \n",
    "                                 max_length=512,\n",
    "                                 task=None,\n",
    "                                 label_list=None,\n",
    "                                 output_mode=None,\n",
    "                                 pad_on_left=False,\n",
    "                                 pad_token=0,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True,):\n",
    "    features = []\n",
    "    label_map = {label: i for i, label in enumerate(np.unique(labels))}\n",
    "    for (ex_index, (example,label)) in enumerate(zip(tqdm(examples),labels)):\n",
    "        inputs = tokenizer.encode_plus(example,add_special_tokens = True,max_length=max_length)\n",
    "        input_ids, token_type_ids, attention_mask = inputs[\"input_ids\"], inputs[\"token_type_ids\"], inputs['attention_mask']\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        features.append(InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=label_map[label]))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 411091/560000 [03:20<01:07, 2196.47it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3fe04f4ef2c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.cache'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-ecd649d81e4d>\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, labels, tokenizer, max_length, task, label_list, output_mode, pad_on_left, pad_token, pad_token_segment_id, mask_padding_with_zero)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madd_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpadding_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_techniques/venv/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m             \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m         )\n\u001b[1;32m   1556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nlp_techniques/venv/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_convert_encoding\u001b[0;34m(encoding, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask)\u001b[0m\n\u001b[1;32m   1497\u001b[0m     ):\n\u001b[1;32m   1498\u001b[0m         encoding_dict = {\n\u001b[0;32m-> 1499\u001b[0;31m             \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m         }\n\u001b[1;32m   1501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, do_lower_case=True,cache_dir='.cache')\n",
    "features = convert_examples_to_features(train_df['text'].values,train_df['label'].values,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                        BPETokenizer,\n",
    "                        SentencePieceBPETokenizer,\n",
    "                        BertWordPieceTokenizer)\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_files_names??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensors and build dataset\n",
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "train_dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataladers\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using BertForSequenceClassification I wanted to implement a network myself\n",
    "class BertWithLClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name,)\n",
    "        self.config = bert.config\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit/test functions\n",
    "def fit(iterator, model, optimizer, criterion,scheduler):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    all_y = []\n",
    "    all_y_hat = []\n",
    "    for step, batch in enumerate(tqdm(train_dataloader,'training epoch')):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1],'token_type_ids':batch[2], \"labels\": batch[3]}\n",
    "        optimizer.zero_grad()\n",
    "        y = inputs['labels']\n",
    "        y_hat = model(**inputs)\n",
    "        loss = criterion(y_hat, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        #TODO YONIGO - whys is this not in epoch granularity?\n",
    "        scheduler.stop()\n",
    "        \n",
    "        all_y.append(y)\n",
    "        all_y_hat.append(y_hat)\n",
    "        \n",
    "    y = torch.cat(all_y,dim=0)\n",
    "    y_hat = torch.cat(all_y_hat,dim=0)\n",
    "    acc = accuracy_score(y.cpu(),y_hat.argmax(1).detach().cpu())\n",
    "    return train_loss / len(iterator.dataset), acc\n",
    "\n",
    "def test(iterator, model, criterion):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.eval()\n",
    "    all_y = []\n",
    "    all_y_hat = []\n",
    "    for step, batch in enumerate(tqdm(train_dataloader,'test epoch')):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1],'token_type_ids':batch[2], \"labels\": batch[3]}\n",
    "        y = inputs['labels']\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(**inputs)\n",
    "        loss = criterion(y_hat, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        all_y.append(y)\n",
    "        all_y_hat.append(y_hat)\n",
    "    y = torch.cat(all_y,dim=0)\n",
    "    y_hat = torch.cat(all_y_hat,dim=0)\n",
    "    acc = accuracy_score(y.cpu(),y_hat.argmax(1).detach().cpu())\n",
    "    return train_loss / len(iterator.dataset), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_epochs(model, n, optimizer, scheduler, train_iterator, valid_iterator):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    for epoch in range(n):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = fit(train_iterator, model, optimizer, criterion,scheduler)\n",
    "        valid_loss, valid_acc = test(valid_iterator, model, criterion)\n",
    "\n",
    "        secs = int(time.time() - start_time)\n",
    "        mins = secs / 60\n",
    "        secs = secs % 60\n",
    "\n",
    "        print('Epoch: %d' % (epoch), \" | time in %d minutes, %d seconds\" % (mins, secs))\n",
    "        print(f'\\tTrain Loss: {train_loss:.4f}\\t|\\tAccuracy: {train_acc :.6f}')\n",
    "        print(f'\\tValidation Loss: {valid_loss:.4f}\\t|\\tAccuracy: {valid_acc:.6f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_optimizer(model, t_total, lr=5e-5, adam_epsilon=1e-8, wd=0, warmup_steps=0):\n",
    "     # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": wd,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    return optimizer,scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 100%|██████████| 1/1 [00:11<00:00, 11.00s/it]\n",
      "test epoch: 100%|██████████| 1/1 [00:02<00:00,  2.82s/it]\n",
      "training epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  | time in 0 minutes, 13 seconds\n",
      "\tTrain Loss: 0.2515\t|\tAccuracy: 0.600000\n",
      "\tValidation Loss: 0.0809\t|\tAccuracy: 0.800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 100%|██████████| 1/1 [00:10<00:00, 10.65s/it]\n",
      "test epoch: 100%|██████████| 1/1 [00:02<00:00,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 13 seconds\n",
      "\tTrain Loss: 0.1075\t|\tAccuracy: 0.600000\n",
      "\tValidation Loss: 0.0781\t|\tAccuracy: 0.800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertWithLClassifier()\n",
    "num_train_epochs=2\n",
    "t_total = len(train_dataloader) // 1 * num_train_epochs\n",
    "optimizer,scheduler = prepare_optimizer(model,t_total)\n",
    "train_n_epochs(model,num_train_epochs,optimizer,scheduler,train_dataloader,train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1611,  1.0176],\n",
      "        [-0.0727,  1.0819],\n",
      "        [-0.2314,  1.4416],\n",
      "        [-0.3342,  1.3276],\n",
      "        [-0.4491,  0.4912]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1],'token_type_ids':batch[2], \"labels\": batch[3]}\n",
    "    y_hat = model(**inputs)\n",
    "    print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_tech",
   "language": "python",
   "name": "nlp_tech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
