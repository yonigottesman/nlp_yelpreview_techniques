{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind of a simplidied version of glue.py and run_glue.py from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import pandas as pd\n",
    "from torchtext import datasets\n",
    "import time\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert imports\n",
    "# To import BertTokenizerFast i must install from src\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers import BertForSequenceClassification,BertModel,BertConfig,BertTokenizerFast,BertTokenizer,AdamW,get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yelp_review_polarity_csv/\n",
      "yelp_review_polarity_csv/readme.txt\n",
      "yelp_review_polarity_csv/test.csv\n",
      "yelp_review_polarity_csv/train.csv\n"
     ]
    }
   ],
   "source": [
    "# Download yelp  polarity\n",
    "torchtext.utils.download_from_url(datasets.text_classification.URLS['YelpReviewPolarity'])\n",
    "!tar -C .data -xvf .data/yelp_review_polarity_csv.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('.data/yelp_review_polarity_csv/train.csv',names=['label','text'])\n",
    "test_df = pd.read_csv('.data/yelp_review_polarity_csv/test.csv',names=['label','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name='bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO YONIGO - where is this cached?\n",
    "#config = BertConfig.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples,\n",
    "                                 labels,\n",
    "                                 tokenizer,          \n",
    "                                 max_length=512,\n",
    "                                 task=None,\n",
    "                                 label_list=None,\n",
    "                                 output_mode=None,\n",
    "                                 pad_on_left=False,\n",
    "                                 pad_token=0,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True,):\n",
    "    features = []\n",
    "    label_map = {label: i for i, label in enumerate(np.unique(labels))}\n",
    "    for (ex_index, (example,label)) in enumerate(zip(tqdm(examples),labels)):\n",
    "        inputs = tokenizer.encode_plus(example,add_special_tokens = True,max_length=max_length)\n",
    "        input_ids, token_type_ids, attention_mask = inputs[\"input_ids\"], inputs[\"token_type_ids\"], inputs['attention_mask']\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        features.append(InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=label_map[label]))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:38<00:00, 2618.01it/s]\n",
      "100%|██████████| 38000/38000 [00:15<00:00, 2483.56it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, do_lower_case=True,cache_dir='.cache',max_length=128)\n",
    "train_features = convert_examples_to_features(train_df['text'][0:100000].values,train_df['label'][0:100000].values,tokenizer)\n",
    "test_features = convert_examples_to_features(test_df['text'].values,test_df['label'].values,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(features):\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "    return dataset\n",
    "train_dataset = create_dataset(train_features)\n",
    "test_dataset = create_dataset(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using BertForSequenceClassification I wanted to implement a network myself\n",
    "class BertWithLClassifier(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name,cache_dir='.cache')\n",
    "        config = self.bert.config\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, output_size)\n",
    "\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def freeze_bert(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_bert(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit/test functions\n",
    "def fit(iterator, model, optimizer, criterion,scheduler,gradient_accumulation_steps):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    all_y = []\n",
    "    all_y_hat = []\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader,'training epoch')):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\"input_ids\": batch[0].to(device), \"attention_mask\": batch[1].to(device),'token_type_ids':batch[2].to(device), \"labels\": batch[3].to(device)}\n",
    "        y = inputs['labels'].to(device)\n",
    "        y_hat = model(**inputs)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule. TODO YONIGO - why not every epoch?\n",
    "            model.zero_grad()\n",
    "            global_step+=1\n",
    "        all_y.append(y)\n",
    "        all_y_hat.append(y_hat)\n",
    "        \n",
    "    y = torch.cat(all_y,dim=0)\n",
    "    y_hat = torch.cat(all_y_hat,dim=0)\n",
    "    acc = accuracy_score(y.cpu(),y_hat.argmax(1).detach().cpu())\n",
    "    return train_loss / global_step, acc\n",
    "\n",
    "def test(iterator, model, criterion):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.eval()\n",
    "    all_y = []\n",
    "    all_y_hat = []\n",
    "    for step, batch in enumerate(tqdm(train_dataloader,'test epoch')):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\"input_ids\": batch[0].to(device), \"attention_mask\": batch[1].to(device),'token_type_ids':batch[2].to(device), \"labels\": batch[3].to(device)}\n",
    "        y = inputs['labels'].to(device)\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(**inputs)\n",
    "        loss = criterion(y_hat, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        all_y.append(y)\n",
    "        all_y_hat.append(y_hat)\n",
    "    y = torch.cat(all_y,dim=0)\n",
    "    y_hat = torch.cat(all_y_hat,dim=0)\n",
    "    acc = accuracy_score(y.cpu(),y_hat.argmax(1).detach().cpu())\n",
    "    return train_loss / len(iterator.dataset), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_epochs(model, n, optimizer, scheduler, train_iterator, valid_iterator,gradient_accumulation_steps):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    for epoch in range(n):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = fit(train_iterator, model, optimizer, criterion,scheduler,gradient_accumulation_steps)\n",
    "        valid_loss, valid_acc = test(valid_iterator, model, criterion)\n",
    "\n",
    "        secs = int(time.time() - start_time)\n",
    "        mins = secs / 60\n",
    "        secs = secs % 60\n",
    "\n",
    "        print('Epoch: %d' % (epoch), \" | time in %d minutes, %d seconds\" % (mins, secs))\n",
    "        print(f'\\tTrain Loss: {train_loss:.4f}\\t|\\tAccuracy: {train_acc :.6f}')\n",
    "        print(f'\\tValidation Loss: {valid_loss:.4f}\\t|\\tAccuracy: {valid_acc:.6f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO YONIGO - maybe clasfifier layer should get bigger learning rate?\n",
    "def prepare_optimizer(model, t_total, lr=5e-5, adam_epsilon=1e-8, wd=0, warmup_steps=0):\n",
    "     # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": wd,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    return optimizer,scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataladers\n",
    "bs=8\n",
    "gradient_accumulation_steps=4\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "test_sampler = RandomSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lables = len(np.unique(train_df['label'].values))\n",
    "model = BertWithLClassifier(num_lables).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 100%|██████████| 12500/12500 [56:19<00:00,  3.70it/s]\n",
      "test epoch: 100%|██████████| 12500/12500 [18:20<00:00, 11.35it/s]\n",
      "training epoch:   0%|          | 0/12500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  | time in 74 minutes, 45 seconds\n",
      "\tTrain Loss: 0.1865\t|\tAccuracy: 0.927950\n",
      "\tValidation Loss: 0.0265\t|\tAccuracy: 0.971950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 100%|██████████| 12500/12500 [56:24<00:00,  3.69it/s]\n",
      "test epoch: 100%|██████████| 12500/12500 [18:23<00:00, 11.33it/s]\n",
      "training epoch:   0%|          | 0/12500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 74 minutes, 54 seconds\n",
      "\tTrain Loss: 0.0926\t|\tAccuracy: 0.968890\n",
      "\tValidation Loss: 0.0092\t|\tAccuracy: 0.991660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 100%|██████████| 12500/12500 [56:25<00:00,  3.69it/s]\n",
      "test epoch: 100%|██████████| 12500/12500 [18:23<00:00, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2  | time in 74 minutes, 54 seconds\n",
      "\tTrain Loss: 0.0366\t|\tAccuracy: 0.990130\n",
      "\tValidation Loss: 0.0033\t|\tAccuracy: 0.997680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_train_epochs=3\n",
    "t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "optimizer,scheduler = prepare_optimizer(model,t_total)\n",
    "train_n_epochs(model,num_train_epochs,optimizer,scheduler,train_dataloader,test_dataloader,gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '.saved_models/bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "74+75+75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
